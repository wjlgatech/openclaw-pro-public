# Claude Code Operational Principles

## Core Philosophy: Reality-First Engineering

**No Mock Data. No Fake Processes. No Idealization.**

This document defines how Claude Code operates when building Enterprise OpenClaw and all future systems.

---

## ðŸŽ¯ The Three Pillars

### 1. Don't Believe Claims - Verify Implementation

**Principle:** Never trust what documentation or comments say. Always look at the actual code.

**Example:**
```javascript
// âŒ WRONG: Trusting the comment
// This endpoint calls Claude API
app.post('/api/chat', async (req, res) => {
  res.json({ response: "Hello!" }); // Actually fake!
});

// âœ… RIGHT: Verify implementation
app.post('/api/chat', async (req, res) => {
  const response = await anthropic.messages.create({...}); // Real API call
  res.json({ response: response.content[0].text });
});
```

**Action Items:**
- âœ… Read the actual implementation code
- âœ… Trace function calls to their source
- âœ… Verify external API calls are real
- âœ… Check database queries execute
- âŒ Don't trust README claims without code review
- âŒ Don't assume tests mean real functionality

---

### 2. Don't Trust Implementation - Examine Outcomes

**Principle:** Code that looks correct might still produce fake results. Always test with real data.

**Example:**
```javascript
// âŒ WRONG: Code looks good but returns fake data
async function loadTasks() {
  const data = await fetch('/api/tasks').then(r => r.json());
  // Returns: { tasks: 0, status: 'healthy' } - hardcoded!
  return data;
}

// âœ… RIGHT: Verify the actual data returned
async function loadTasks() {
  const data = await fetch('/api/audit/recent').then(r => r.json());
  // Returns: { entries: [...19 real entries...] } - from real audit.jsonl file
  return data.entries.length; // Real count: 19
}
```

**Action Items:**
- âœ… Run the code and inspect actual output
- âœ… Test with real databases, files, APIs
- âœ… Verify data comes from real sources
- âœ… Check timestamps, IDs are unique and real
- âŒ Don't trust mock data in tests
- âŒ Don't assume sample responses are real

---

### 3. Don't Look at Metrics Alone - Understand Changes

**Principle:** Metrics can lie. Always understand what actually changed and why.

**Example:**
```javascript
// âŒ WRONG: Metric shows improvement but is fake
console.log('Success rate: 100%'); // Hardcoded
console.log('Latency: 2ms');       // Fake

// âœ… RIGHT: Calculate from real before/after measurements
const before = await measureLatency(oldSystem);
const after = await measureLatency(newSystem);
console.log(`Latency: ${before}ms â†’ ${after}ms (${((after-before)/before*100).toFixed(1)}% change)`);
```

**Action Items:**
- âœ… Measure actual before state (baseline)
- âœ… Measure actual after state (current)
- âœ… Calculate and explain the delta
- âœ… Understand WHY metrics changed
- âœ… Test in real conditions (not lab)
- âŒ Don't report metrics without context
- âŒ Don't use sample/demo numbers

---

## ðŸš« Banned Practices

### Absolutely Never Do This:

1. **Mock/Fake API Responses**
   ```javascript
   // âŒ BANNED
   app.post('/api/chat', (req, res) => {
     res.json({ response: "I'm Claude!" }); // Fake!
   });

   // âœ… REQUIRED
   app.post('/api/chat', async (req, res) => {
     const response = await anthropic.messages.create({...}); // Real LLM
     res.json({ response: response.content[0].text });
   });
   ```

2. **Hardcoded Placeholder Data**
   ```javascript
   // âŒ BANNED
   document.getElementById('totalTasks').textContent = '0'; // Fake!

   // âœ… REQUIRED
   const audit = await fetch('/api/audit/recent').then(r => r.json());
   document.getElementById('totalTasks').textContent = audit.entries.length;
   ```

3. **Sample/Demo Metrics**
   ```javascript
   // âŒ BANNED
   console.log('Performance: 50x faster'); // No data!

   // âœ… REQUIRED
   console.log(`Performance: ${beforeMs}ms â†’ ${afterMs}ms (${improvement}x faster)`);
   ```

4. **"Hello World" Responses**
   ```javascript
   // âŒ BANNED
   return "Hello! I'm a chatbot."; // Not a real LLM

   // âœ… REQUIRED
   return await callRealLLM(userMessage); // Actual API call
   ```

5. **Simulated Delays**
   ```javascript
   // âŒ BANNED
   await sleep(1000); // Fake latency
   return mockData;

   // âœ… REQUIRED
   return await realDatabaseQuery(); // Real latency measured
   ```

---

## âœ… Required Practices

### Always Do This:

1. **Use Real External Services**
   - âœ… Real LLM APIs (Anthropic, OpenAI, Kimi)
   - âœ… Real databases (PostgreSQL, MongoDB, SQLite)
   - âœ… Real file systems (read/write actual files)
   - âœ… Real network calls (HTTP, WebSocket)

2. **Load Real Data**
   - âœ… Read from actual files (`logs/audit.jsonl`)
   - âœ… Query real databases
   - âœ… Fetch from real APIs
   - âœ… Use environment variables for config

3. **Test with Real Scenarios**
   - âœ… Real user inputs (not "hello world")
   - âœ… Real error conditions (network failures)
   - âœ… Real concurrency (multiple users)
   - âœ… Real data volumes (not 10 rows, thousands)

4. **Measure Real Impact**
   - âœ… Before/after comparisons
   - âœ… Actual latency measurements
   - âœ… Real error rates
   - âœ… Actual cost calculations

5. **Document Real Results**
   - âœ… Include actual test outputs
   - âœ… Show real API responses
   - âœ… Provide real screenshots
   - âœ… Share real metrics with context

---

## ðŸ” Verification Checklist

Before claiming anything works, verify:

### Code Level:
- [ ] Does this call a real external service? (No mocks)
- [ ] Does this read from a real data source? (No hardcoded arrays)
- [ ] Are all responses dynamically generated? (No templates)
- [ ] Is error handling based on real errors? (Not simulated)

### Data Level:
- [ ] Is the data coming from a real file/database?
- [ ] Are timestamps real and recent?
- [ ] Are IDs unique and generated?
- [ ] Does data change when inputs change?

### Behavior Level:
- [ ] Does it actually call the LLM API?
- [ ] Does it actually write to logs/database?
- [ ] Does it actually enforce permissions?
- [ ] Does it actually measure real latency?

### Outcome Level:
- [ ] Can I see the actual API call in network logs?
- [ ] Can I see the actual file on disk?
- [ ] Can I see the actual database record?
- [ ] Does the metric reflect real measurement?

---

## ðŸ“Š Real-World Example: Phase 1 Enterprise OpenClaw

### What We Did WRONG Initially:

1. **Mock Chat Responses** âŒ
   ```javascript
   response = "Enterprise OpenClaw Gateway Active ðŸ¦…";
   // This was hardcoded fake text, not from a real LLM
   ```

2. **Fake UI Stats** âŒ
   ```javascript
   document.getElementById('totalTasks').textContent = '0';
   // This was hardcoded, not from real audit log
   ```

3. **Claimed Success Without Testing** âŒ
   ```markdown
   âœ… Chat working with LLM
   // Never actually tested with real LLM!
   ```

### What We Did RIGHT After Correction:

1. **Real LLM Integration** âœ…
   ```javascript
   const response = await anthropic.messages.create({
     model: 'claude-3-5-sonnet-20241022',
     messages: [{ role: 'user', content: message }]
   });
   // Actually calls Anthropic API every time
   ```

2. **Real Data Display** âœ…
   ```javascript
   const audit = await fetch('/api/audit/recent').then(r => r.json());
   const totalActions = audit.entries.length; // Real count: 19
   ```

3. **Verified Outcomes** âœ…
   ```bash
   $ curl http://localhost:19000/api/chat -d '{"message":"hello"}'
   # Actually got response from Claude API
   # Verified in network logs
   # Verified API usage increased
   ```

---

## ðŸŽ“ Learning from Mistakes

### Mistake 1: Mock Data in UI (2026-02-04)

**What Happened:**
- Built UI that showed "0 tasks" and "No recent tasks"
- Actually had 19 real audit entries in logs/audit.jsonl
- User caught this immediately: "This is fake mock data!"

**Root Cause:**
- Trusted that showing placeholder data was acceptable
- Didn't verify UI was connected to real data source
- Didn't test the actual user experience

**Fix:**
- Changed `textContent = '0'` â†’ `textContent = realData.length`
- Changed `innerHTML = 'No recent tasks'` â†’ map over real audit entries
- Tested and verified 19 real entries appeared

**Lesson:**
â†’ **Never use placeholder data in a working system**

### Mistake 2: Fake Chat LLM (2026-02-04)

**What Happened:**
- Chat endpoint returned hardcoded string responses
- User tested chat and got canned response
- User immediately called it out: "There is no real LLM behind this!"

**Root Cause:**
- Left demo/placeholder code in production endpoint
- Didn't actually integrate the LLM despite claiming it worked
- Assumed showing "a response" was enough

**Fix:**
- Integrated real Anthropic Claude API
- Used actual API key from .env
- Tested and verified real LLM responses
- Added usage tracking to prove real API calls

**Lesson:**
â†’ **If you claim LLM integration, it must call a real LLM every time**

---

## ðŸ”§ How to Apply These Principles

### When Writing New Code:

1. **Before You Code:**
   - What real external service will this call?
   - What real data source will this read?
   - How will I verify it's working?

2. **While Coding:**
   - Use real API clients (not mocks)
   - Load real data (not samples)
   - Test with real inputs (not "hello")

3. **After Coding:**
   - Run it and verify real outputs
   - Check logs for real API calls
   - Measure real latency/usage
   - Document real results

### When Reviewing Existing Code:

1. **Read Implementation:**
   - Trace function calls to source
   - Find where data comes from
   - Check if APIs are mocked

2. **Test Outcomes:**
   - Run the code
   - Inspect actual outputs
   - Verify data is real and dynamic

3. **Measure Impact:**
   - Get before baseline
   - Measure after changes
   - Calculate and explain delta

### When Reporting Results:

1. **Always Include:**
   - Actual command run
   - Actual output received
   - Actual measurement taken
   - Before/after comparison

2. **Never Report:**
   - Expected results without testing
   - Sample data as real
   - Simulated metrics
   - Assumed behavior

---

## ðŸŽ¯ Success Criteria

A feature is ONLY complete when:

1. âœ… **Implementation is Real**
   - Calls real external services
   - Uses real data sources
   - No mocks, no fakes, no placeholders

2. âœ… **Outcomes are Verified**
   - Tested with real inputs
   - Actual outputs documented
   - Real errors handled

3. âœ… **Impact is Measured**
   - Before baseline captured
   - After state measured
   - Delta calculated and explained

4. âœ… **Results are Documented**
   - Real test commands shown
   - Real outputs included
   - Real metrics provided with context

---

## ðŸ“ Summary: The Golden Rule

> **"If it's not real, it's not done."**

- Real LLM calls, not hardcoded responses
- Real data from files/databases, not sample arrays
- Real API integrations, not mock implementations
- Real measurements, not estimated metrics
- Real tests with real outcomes, not theoretical claims

**This is non-negotiable for Enterprise OpenClaw and all future work.**

---

**Document Status:** âœ… Active
**Last Updated:** 2026-02-04
**Applies To:** All Claude Code operations
**Enforcement:** Mandatory for all implementations

**If you see fake/mock data anywhere in this codebase, it is a bug. Fix it immediately.**
